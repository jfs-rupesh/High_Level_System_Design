![ System Architecture](../diagrams/rate.png)
![ System Architecture](../diagrams/rate2.png)



# âš¡ï¸ High-Performance Rate Limiter â€“ Design & Implementation

## ğŸ“˜ Overview

This project implements a **distributed, low-latency rate limiter** that:

* Prevents abuse of API endpoints
* Supports multiple algorithms
* Minimizes impact on backend services
* Scales independently and reliably

Built to handle **billions of users and millions of requests per second**, it includes:

* Fixed and Sliding Window algorithms
* Redis-based state store with single-leader replication
* Optional load balancer-level caching for popular clients

---

## ğŸ“Œ Core Features

* ğŸ”’ Prevents overuse of public/private API endpoints
* ğŸ“Š Works by user ID or IP address
* ğŸ§  Supports multiple algorithms:

  * Fixed Window
  * Sliding Window
* ğŸ§  Thread-safe with locking/concurrent data structures
* âš™ In-memory store (Redis) for low-latency checks
* ğŸ” Horizontally scalable with partitioned Redis shards

---

## ğŸ“ˆ Capacity & Scaling

* ğŸ§‘â€ğŸ¤â€ğŸ§‘ **1 Billion Users**
* ğŸš€ **20 Services per User**
* ğŸ’¾ Est. Storage: `~240 GB` of counters (user Ã— services Ã— metadata)
* ğŸ§  Stored in Redis partitions, replicated for fault tolerance

---

## ğŸ“ Design Goals

| Goal           | Strategy                                |
| -------------- | --------------------------------------- |
| â± Low latency  | In-memory store (Redis) + caching       |
| ğŸ’ª Scalability | Partitioned Redis + horizontal services |
| ğŸ¯ Accuracy    | Single-leader writes, optional CRDTs    |
| ğŸ§© Flexibility | Pluggable rate-limiting algorithms      |
| ğŸ” Resilience  | Backup Redis replicas, retry logic      |

---

## âš™ Rate Limiting API

```ts
interface RateLimiter {
  boolean rateLimit(
    String? userId,
    String ipAddress,
    String serviceName,
    long timestamp
  )
}
```

* `userId`: Nullable (optional for public endpoints)
* `ipAddress`: Always available for fallback
* `serviceName`: Name of the API being limited
* `timestamp`: For algorithmic tracking

---

## ğŸ§  Algorithms Implemented

### 1. ğŸ§Š Fixed Window

```text
Rate = 2 requests / minute
Window = [1:00 to 1:01)
Valid: 2 requests in that interval
Reset: counter = 0 at 1:01
```

**State format:**

```ts
Map<Service, Map<User, (MinuteWindow, Count)>>
```

**Pros:**

* Simple and memory efficient

**Cons:**

* Allows bursts at window edges (e.g., 2 at 12:00:59 and 2 at 12:01:00)

---

### 2. ğŸªŸ Sliding Window

```text
Rate = 2 requests / 60s
Window = current time - 60s
LinkedList = Timestamps of past requests
Purge old entries, count remaining
```

**Data structure:**

```ts
Map<Service, Map<User, Deque<Timestamp>>>
```

**Pros:**

* Smoother, fairer throttling

**Cons:**

* Slightly more memory intensive

---

## ğŸ—ƒ Storage Design â€“ Redis

### Chosen: **Redis** with **Single-Leader Replication**

* âš¡ Blazing fast reads/writes
* âœ… Built-in data structures (List, HashMap)
* ğŸ§  Easy to replicate
* ğŸ’¥ Supports failover with Sentinel or Cluster mode

---

## ğŸ— Architecture

```text
Client
  â†“
Load Balancer (w/ optional local rate cache)
  â†“
Rate Limiter Service
  â†“
Redis Partition (Sharded by user/IP)
  â†“
Backend API (if allowed)
```

### Caching Strategy (optional)

* For "hot" users/IPs
* Load balancer can locally track request counts
* Reduces Redis hits for heavy clients

---

## ğŸ”€ Replication Models Considered

| Model             | Verdict  | Why                                |
| ----------------- | -------- | ---------------------------------- |
| Single Leader     | âœ… Chosen | Accurate counts, simple failover   |
| Multi-Leader      | âŒ No     | Delay in consistency, merge issues |
| Leaderless (CRDT) | âŒ No     | Inexact counts, high complexity    |

---

## ğŸ’¡ Placement Tradeoffs

| Approach               | Pros                              | Cons                                        |
| ---------------------- | --------------------------------- | ------------------------------------------- |
| **Inline in service**  | No extra latency, simple          | Doesnâ€™t block spam traffic, scales with app |
| **Standalone service** | Decoupled, centralized, cacheable | Extra hop in request path                   |
| **LB cache hybrid**    | Shields Redis from hot users      | Complex invalidation, consistency           |

---

## ğŸš¦ Thread Safety

### Issues Handled

* Atomic increments (for counters)
* Concurrent writes to lists
* Lost updates / race conditions

### Fixes

* Use Redis atomic ops (e.g., `INCR`)
* Use thread-safe collections if in app memory
* Synchronization/locking around critical sections

---

## ğŸ“¦ Deployment Strategy

* Redis Clusters with partitioned keyspace (by user/IP hash)
* Multiple rate limiter services, stateless
* Load balancers round-robin or hash route traffic
* Cache layer at LB (optional)

---

## ğŸ” Example Use Case

```http
POST /api/v1/post-comment
User: 12345
IP: 192.168.1.10

â†’ Load Balancer checks cache
â†’ If not limited, call RateLimiter Service
â†’ RateLimiter hits Redis partition
â†’ Pass/fail based on limit
â†’ Route to comment service if allowed
```

---

## âœ… TL;DR

| Feature         | Summary                                    |
| --------------- | ------------------------------------------ |
| Rate Algorithms | Fixed window, sliding window               |
| DB Backend      | Redis (partitioned, single-leader)         |
| Fault Tolerance | Redis replication, load-balanced services  |
| Caching         | Optional hot-client cache in load balancer |
| Scalability     | Horizontal (services + Redis partitions)   |
| Use cases       | API protection, abuse prevention, fairness |

---


